{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8623856,"sourceType":"datasetVersion","datasetId":5162780},{"sourceId":8638343,"sourceType":"datasetVersion","datasetId":5173106}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install openai langchain langchain_community unstructured yt_dlp --upgrade --quiet  docx2txt tiktoken chromadb","metadata":{"execution":{"iopub.status.busy":"2024-06-08T09:24:53.266902Z","iopub.status.idle":"2024-06-08T09:24:53.267284Z","shell.execute_reply.started":"2024-06-08T09:24:53.267110Z","shell.execute_reply":"2024-06-08T09:24:53.267125Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sys\nimport os\nimport openai\nsys.path.append('../..')\n\nfrom dotenv import load_dotenv, find_dotenv\n\n_ = load_dotenv(find_dotenv())               # find and load the local env file\n\nos.environ['OPENAI_API_KEY'] = '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'\nopenai.api_key = os.environ['OPENAI_API_KEY']","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-08T09:53:03.232179Z","iopub.execute_input":"2024-06-08T09:53:03.233071Z","iopub.status.idle":"2024-06-08T09:53:03.239927Z","shell.execute_reply.started":"2024-06-08T09:53:03.233033Z","shell.execute_reply":"2024-06-08T09:53:03.238645Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"# DOCUMENT LOADING","metadata":{}},{"cell_type":"markdown","source":"## Working with PDFs","metadata":{}},{"cell_type":"code","source":"from langchain_community.document_loaders import PyPDFLoader\nfrom langchain_community.document_loaders import Docx2txtLoader\n\nloader = Docx2txtLoader('/kaggle/input/llmtext/LLMs.docx')\npages = loader.load()\nlen(pages)","metadata":{"execution":{"iopub.status.busy":"2024-06-08T09:24:17.696279Z","iopub.execute_input":"2024-06-08T09:24:17.696843Z","iopub.status.idle":"2024-06-08T09:24:18.081381Z","shell.execute_reply.started":"2024-06-08T09:24:17.696728Z","shell.execute_reply":"2024-06-08T09:24:18.080252Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"1"},"metadata":{}}]},{"cell_type":"code","source":"page = pages[0]\ndash = \"-\"*100\nprint(f\"First 500 chars \\n{dash} \\n\",page.page_content[0: 500], f\"\\n{dash}\")\n\nprint(\"\\nMetadata \\n\\n\", page.metadata)","metadata":{"execution":{"iopub.status.busy":"2024-06-08T09:24:18.083589Z","iopub.execute_input":"2024-06-08T09:24:18.083927Z","iopub.status.idle":"2024-06-08T09:24:18.089881Z","shell.execute_reply.started":"2024-06-08T09:24:18.083896Z","shell.execute_reply":"2024-06-08T09:24:18.088698Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"First 500 chars \n---------------------------------------------------------------------------------------------------- \n LARGE LANGUAGE MODELS\n\nIt is deep learning algorithm that can perform a variety of NLP tasks.  They use transformer models and are trained on massive datasets – hence the term “Large”. Large language models must be pre-trained and then fine-tuned so that they can solve text classification, question answering, document summarization, and text generation problems.\n\nNLP applications of LLMs – translation, chatbots, AI assistants, etc.\n\n\n\nPopular pre-trained language models –\n\n1) BERT – Google\n\n2) G \n----------------------------------------------------------------------------------------------------\n\nMetadata \n\n {'source': '/kaggle/input/llmtext/LLMs.docx'}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Audio Loader","metadata":{}},{"cell_type":"code","source":"from langchain.document_loaders.generic import GenericLoader\nfrom langchain.document_loaders.parsers import OpenAIWhisperParser\nfrom langchain.document_loaders.blob_loaders.youtube_audio import YoutubeAudioLoader\n\nurl = 'https://www.youtube.com/watch?v=zBjJUV-lzHo'\nsave_dir = '/kaggle/working/youtube/'\n\nloader = GenericLoader(YoutubeAudioLoader([url], save_dir), OpenAIWhisperParser())\ndocs = loader.load()","metadata":{"execution":{"iopub.status.busy":"2024-06-08T09:24:18.091325Z","iopub.execute_input":"2024-06-08T09:24:18.092070Z","iopub.status.idle":"2024-06-08T09:24:48.757222Z","shell.execute_reply.started":"2024-06-08T09:24:18.092036Z","shell.execute_reply":"2024-06-08T09:24:48.755899Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"[youtube] Extracting URL: https://www.youtube.com/watch?v=zBjJUV-lzHo\n[youtube] zBjJUV-lzHo: Downloading webpage\n[youtube] zBjJUV-lzHo: Downloading ios player API JSON\n[youtube] zBjJUV-lzHo: Downloading player dee49cfa\n[youtube] zBjJUV-lzHo: Downloading m3u8 information\n[info] zBjJUV-lzHo: Downloading 1 format(s): 140\n[download] Destination: /kaggle/working/youtube//1 minute funny videos.m4a\n[download] 100% of  950.34KiB in 00:00:00 at 6.69MiB/s   \n[FixupM4a] Correcting container of \"/kaggle/working/youtube//1 minute funny videos.m4a\"\n[ExtractAudio] Not converting audio /kaggle/working/youtube//1 minute funny videos.m4a; file is already in target format m4a\nTranscribing part 1!\nAttempt 1 failed. Exception: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\nAttempt 2 failed. Exception: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\nAttempt 3 failed. Exception: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\nFailed to transcribe after 3 attempts.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Web Loader","metadata":{}},{"cell_type":"code","source":"from langchain.document_loaders import WebBaseLoader\n\nloader = WebBaseLoader('https://towardsdatascience.com/recommender-systems-a-complete-guide-to-machine-learning-models-96d3f94ea748')\ndocs = loader.load()\nlen(docs)","metadata":{"execution":{"iopub.status.busy":"2024-06-08T09:24:48.758834Z","iopub.execute_input":"2024-06-08T09:24:48.759208Z","iopub.status.idle":"2024-06-08T09:24:50.614517Z","shell.execute_reply.started":"2024-06-08T09:24:48.759173Z","shell.execute_reply":"2024-06-08T09:24:50.613273Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"1"},"metadata":{}}]},{"cell_type":"code","source":"print(docs[0].page_content[:500])","metadata":{"execution":{"iopub.status.busy":"2024-06-08T09:24:50.616259Z","iopub.execute_input":"2024-06-08T09:24:50.616793Z","iopub.status.idle":"2024-06-08T09:24:50.622219Z","shell.execute_reply.started":"2024-06-08T09:24:50.616760Z","shell.execute_reply":"2024-06-08T09:24:50.621033Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Recommender Systems — A Complete Guide to Machine Learning Models | by Francesco Casalegno | Towards Data ScienceOpen in appSign upSign inWriteSign upSign inRecommender Systems — A Complete Guide to Machine Learning ModelsLeveraging data to help users discovering new contentFrancesco Casalegno·FollowPublished inTowards Data Science·10 min read·Nov 25, 2022--2ListenSharePhoto by Javier Allegue Barros on UnsplashRecommender Systems: Why And How?Recommender systems are algorithms providing personal\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# DOCUMENT SPLITTING","metadata":{}},{"cell_type":"code","source":"'''\nList of splitters -\n- CharacterTextSplitter\n- MarkdownHeaderTextSplitter\n- TokenTextSplitter\n- SentenceTransformersTokenTextSplitter\n- RecursiveCharacterTextSplitter\n- NLTKTextSplitter\n- SpacyTextSplitter\n- Language - (for programming languages)\n'''\n\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter, MarkdownHeaderTextSplitter\n\nchunk_size = 26\nchunk_overlap = 4\n\nr_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\nc_splitter = CharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap, separator=' ')","metadata":{"execution":{"iopub.status.busy":"2024-06-08T09:24:50.623528Z","iopub.execute_input":"2024-06-08T09:24:50.623825Z","iopub.status.idle":"2024-06-08T09:24:50.642487Z","shell.execute_reply.started":"2024-06-08T09:24:50.623798Z","shell.execute_reply":"2024-06-08T09:24:50.641442Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"text1 = 'abcdefghijklmnopqrstuvvdkfbfdsKbfdhBF'\nr_splitter.split_text(text1)","metadata":{"execution":{"iopub.status.busy":"2024-06-08T09:24:50.643537Z","iopub.execute_input":"2024-06-08T09:24:50.643831Z","iopub.status.idle":"2024-06-08T09:24:50.659762Z","shell.execute_reply.started":"2024-06-08T09:24:50.643806Z","shell.execute_reply":"2024-06-08T09:24:50.658445Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"['abcdefghijklmnopqrstuvvdkf', 'vdkfbfdsKbfdhBF']"},"metadata":{}}]},{"cell_type":"code","source":"text2 = 'abcdefghijklmnopqrs tuvwxyzvdkfbf dsKbfdhBF'\nc_splitter.split_text(text2)","metadata":{"execution":{"iopub.status.busy":"2024-06-08T09:24:50.664032Z","iopub.execute_input":"2024-06-08T09:24:50.664794Z","iopub.status.idle":"2024-06-08T09:24:50.671837Z","shell.execute_reply.started":"2024-06-08T09:24:50.664749Z","shell.execute_reply":"2024-06-08T09:24:50.670674Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"['abcdefghijklmnopqrs', 'tuvwxyzvdkfbf dsKbfdhBF']"},"metadata":{}}]},{"cell_type":"code","source":"text3 = 'a b c d e f g h i j k l m n o p q r s t u v w x y z v d k f b f d s K b f d h B F'\nc_splitter.split_text(text3)","metadata":{"execution":{"iopub.status.busy":"2024-06-08T09:24:50.673221Z","iopub.execute_input":"2024-06-08T09:24:50.673639Z","iopub.status.idle":"2024-06-08T09:24:50.684757Z","shell.execute_reply.started":"2024-06-08T09:24:50.673602Z","shell.execute_reply":"2024-06-08T09:24:50.683622Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"['a b c d e f g h i j k l m',\n 'l m n o p q r s t u v w x',\n 'w x y z v d k f b f d s K',\n 's K b f d h B F']"},"metadata":{}}]},{"cell_type":"code","source":"paragraph = 'Recommender systems are algorithms providing personalized suggestions for items that are most relevant to each user. With the massive growth of available online contents, users have been inundated with choices. It is therefore crucial for web platforms to offer recommendations of items to each user, in order to increase user satisfaction and engagement. \\\nThe following list shows examples of well-known web platforms with a huge number of available contents, which need efficient recommender systems to keep users interested. All these platforms use powerful machine learning models in order to generate relevant recommendations for each user. \\\n\\\nIn recommender systems, machine learning models are used to predict the rating rᵤᵢ of a user u on an item i. At inference time, we recommend to each user u the items l having highest predicted rating rᵤᵢ. \\\nWe therefore need to collect user feedback, so that we can have a ground truth for training and evaluating our models. An important distinction has to be made here between explicit feedback and implicit feedback.'\n\nprint(len(paragraph))\n\nc_splitter = CharacterTextSplitter(\n            chunk_size = 150,\n            chunk_overlap = 25,\n            separator = ' ')\n\nr_splitter = RecursiveCharacterTextSplitter(\n            chunk_size = 150,\n            chunk_overlap = 25,\n            separators = [\"\\n\\n\", \"\\n\", \" \", \"\", \".\"])","metadata":{"execution":{"iopub.status.busy":"2024-06-08T09:24:50.686323Z","iopub.execute_input":"2024-06-08T09:24:50.686694Z","iopub.status.idle":"2024-06-08T09:24:50.696416Z","shell.execute_reply.started":"2024-06-08T09:24:50.686664Z","shell.execute_reply":"2024-06-08T09:24:50.695243Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"1062\n","output_type":"stream"}]},{"cell_type":"code","source":"c_splitter.split_text(paragraph)","metadata":{"execution":{"iopub.status.busy":"2024-06-08T09:24:50.697739Z","iopub.execute_input":"2024-06-08T09:24:50.698071Z","iopub.status.idle":"2024-06-08T09:24:50.715019Z","shell.execute_reply.started":"2024-06-08T09:24:50.698020Z","shell.execute_reply":"2024-06-08T09:24:50.713672Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"['Recommender systems are algorithms providing personalized suggestions for items that are most relevant to each user. With the massive growth of',\n 'the massive growth of available online contents, users have been inundated with choices. It is therefore crucial for web platforms to offer',\n 'web platforms to offer recommendations of items to each user, in order to increase user satisfaction and engagement. The following list shows examples',\n 'list shows examples of well-known web platforms with a huge number of available contents, which need efficient recommender systems to keep users',\n 'systems to keep users interested. All these platforms use powerful machine learning models in order to generate relevant recommendations for each',\n 'recommendations for each user. In recommender systems, machine learning models are used to predict the rating rᵤᵢ of a user u on an item i. At',\n 'a user u on an item i. At inference time, we recommend to each user u the items l having highest predicted rating rᵤᵢ. We therefore need to collect',\n 'therefore need to collect user feedback, so that we can have a ground truth for training and evaluating our models. An important distinction has to be',\n 'distinction has to be made here between explicit feedback and implicit feedback.']"},"metadata":{}}]},{"cell_type":"code","source":"r_splitter.split_text(paragraph)","metadata":{"execution":{"iopub.status.busy":"2024-06-08T09:24:50.716701Z","iopub.execute_input":"2024-06-08T09:24:50.717136Z","iopub.status.idle":"2024-06-08T09:24:50.728245Z","shell.execute_reply.started":"2024-06-08T09:24:50.717094Z","shell.execute_reply":"2024-06-08T09:24:50.727109Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"['Recommender systems are algorithms providing personalized suggestions for items that are most relevant to each user. With the massive growth of',\n 'the massive growth of available online contents, users have been inundated with choices. It is therefore crucial for web platforms to offer',\n 'web platforms to offer recommendations of items to each user, in order to increase user satisfaction and engagement. The following list shows',\n 'The following list shows examples of well-known web platforms with a huge number of available contents, which need efficient recommender systems to',\n 'recommender systems to keep users interested. All these platforms use powerful machine learning models in order to generate relevant recommendations',\n 'relevant recommendations for each user. In recommender systems, machine learning models are used to predict the rating rᵤᵢ of a user u on an item i.',\n 'a user u on an item i. At inference time, we recommend to each user u the items l having highest predicted rating rᵤᵢ. We therefore need to collect',\n 'need to collect user feedback, so that we can have a ground truth for training and evaluating our models. An important distinction has to be made',\n 'has to be made here between explicit feedback and implicit feedback.']"},"metadata":{}}]},{"cell_type":"markdown","source":"### Adding metadata to chunks ","metadata":{}},{"cell_type":"code","source":"pages[0].metadata","metadata":{"execution":{"iopub.status.busy":"2024-06-08T09:24:50.729293Z","iopub.execute_input":"2024-06-08T09:24:50.729659Z","iopub.status.idle":"2024-06-08T09:24:50.739350Z","shell.execute_reply.started":"2024-06-08T09:24:50.729627Z","shell.execute_reply":"2024-06-08T09:24:50.738209Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"{'source': '/kaggle/input/llmtext/LLMs.docx'}"},"metadata":{}}]},{"cell_type":"code","source":"''' MarkdownHeaderTextSplitter adds header split metadata to each split. '''\n\nmarkdown_doc = \"\"\" # Title\\n\\n\n## Chapter 1\\n\\n \\\nHi this is Jim\\n\\n Hi this is Joe\\n\\n\\\\\n### Section \\n\\n\\ \\\nHi this is Lance \\n\\n\n## Chapter  2\\n\\n \\\nHi this is Molly\"\"\"\n\nheaders_to_split_on = [\n    (\"#\", \"Header1\"),\n    (\"##\",\"Header2\"),\n    (\"###\",\"Header 3\"),\n]","metadata":{"execution":{"iopub.status.busy":"2024-06-08T09:24:50.740661Z","iopub.execute_input":"2024-06-08T09:24:50.740977Z","iopub.status.idle":"2024-06-08T09:24:50.751015Z","shell.execute_reply.started":"2024-06-08T09:24:50.740947Z","shell.execute_reply":"2024-06-08T09:24:50.749751Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on = headers_to_split_on)\nmd_header_splits = markdown_splitter.split_text(markdown_doc)\n\nprint(md_header_splits[0],\"\\n\")\nprint(md_header_splits[1])","metadata":{"execution":{"iopub.status.busy":"2024-06-08T09:24:50.752197Z","iopub.execute_input":"2024-06-08T09:24:50.752928Z","iopub.status.idle":"2024-06-08T09:24:50.764126Z","shell.execute_reply.started":"2024-06-08T09:24:50.752884Z","shell.execute_reply":"2024-06-08T09:24:50.762924Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"page_content='Hi this is Jim  \\nHi this is Joe  \\n\\\\' metadata={'Header1': 'Title', 'Header2': 'Chapter 1'} \n\npage_content='\\\\ Hi this is Lance' metadata={'Header1': 'Title', 'Header2': 'Chapter 1', 'Header 3': 'Section'}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# VECTOR STORES AND EMBEDDINGS","metadata":{}},{"cell_type":"code","source":"''' A vector store is a database that stores similar embeddings \nso that it is easier to retrieve similar data quickly.'''\n\nloaders = [\n    PyPDFLoader('/kaggle/input/infodata/Soham_Deshmukh_B21EE067_Exp_5.pdf'),\n    PyPDFLoader('/kaggle/input/infodata/Soham_Deshmukh_B21EE067_Expt10.pdf'),\n    PyPDFLoader('/kaggle/input/infodata/Soham_Deshmukh_B21EE067_Expt11.pdf'),\n]\n\ndocs = []\nfor loader in loaders:\n    docs.extend(loader.load())","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-06-08T09:24:50.765788Z","iopub.execute_input":"2024-06-08T09:24:50.766577Z","iopub.status.idle":"2024-06-08T09:24:51.619478Z","shell.execute_reply.started":"2024-06-08T09:24:50.766535Z","shell.execute_reply":"2024-06-08T09:24:51.618388Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"for i in range(5):\n    print(docs[i])\n    print(\"\\n\")","metadata":{"execution":{"iopub.status.busy":"2024-06-08T09:26:00.399799Z","iopub.execute_input":"2024-06-08T09:26:00.400179Z","iopub.status.idle":"2024-06-08T09:26:00.406134Z","shell.execute_reply.started":"2024-06-08T09:26:00.400148Z","shell.execute_reply":"2024-06-08T09:26:00.404931Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"page_content='EEP3010: Communication Systems Lab  \\n \\n \\n \\n \\nEEP3010: Laboratory Report (Experiment 5) \\nOn \\n \\nSTUDY  OF QUADRATURE AMPLITUDE MODULATION AND \\nDEMODULATION  \\n \\n \\n \\nBy \\n \\nName: Soham Niraj Deshmukh (B21EE067)  \\n \\nDate: 22 February 2024  \\n \\n \\n \\n \\n \\n ' metadata={'source': '/kaggle/input/infodata/Soham_Deshmukh_B21EE067_Exp_5.pdf', 'page': 0}\n\n\npage_content='EEP3010: Communication Systems Lab  \\nExperiment 5 \\n \\nAbstract:  \\nIn the experiment, our aim is to observe the - \\ni) Tribit data coding technique  \\nii) Differential encoding  \\niii) Constellation diagram of the Quadrature Amplitude Modulation (8 -QAM) \\nand bandwidth efficiency of the scheme  \\nIn tribit data coding technique, we shall use 3 clock waveforms – Even clock, \\nOdd clock,  and C clock. Each of these clocks are out of phase by the bit time \\nTb. Each  clock has a time -period of 3Tb . The input data is multiplied by the 3 \\nclock signals using a D flip flop to get the 3 encoded values – the Even bit, the \\nOdd bit and the C bit.  \\n \\nIn differential encoding technique , we will use an EXOR logic between the \\ndata to be transmitted and the delayed output of the EXOR. The difference \\nbetween the previous and current value (EXOR of the two values) is the \\nencoded output.  \\n \\nIn Quadrature Amplitude Modulation (QAM), both the magnitude and phase \\nof the carrier signal are changed. To observe the changes, we will use the \\nconstellation diagram, where each point on the diagram represents a unique \\ncombination of amplitude and phase.   \\n' metadata={'source': '/kaggle/input/infodata/Soham_Deshmukh_B21EE067_Exp_5.pdf', 'page': 1}\n\n\npage_content='EEP3010: Communication Systems Lab  \\n \\nThe distance between these points represents the difference in signal \\ncharacteristics, which is used to transmit multiple bits per symbol.   \\nThrough this experiment, we shall first obtain the tribit encoded data for a \\ncertain input sequence – out of the 3 bits, 1 bit corresponds to amplitude \\nand the rest 2 correspond to phase. The modulated signal can take 4  \\ndifferent phases and 2 different amplitudes, for  a total of 8 different states  – \\nhence the name 8 -QAM.  \\n \\n \\n \\nObjective:  \\ni) To study tribit coding techniques for Non -Return to Zero data format  \\nii) To study differential encoding techniques for data  \\niii) O bservation of the constellation diagram for QAM  and study  of \\nbandwidth efficiency of QAM  \\niv) Study of Carrier Modulation and demodulation Techniques by \\nQuadrature Amplitude method  \\n \\nExperimental Procedure:  \\n \\nEquipment Required:  \\nI. Experimental kit DCL -QAM  \\nII. Connecting Chords  \\nIII. Power supply  \\nIV. 20 MHz Dual Trace Oscilloscope  \\n \\n \\n' metadata={'source': '/kaggle/input/infodata/Soham_Deshmukh_B21EE067_Exp_5.pdf', 'page': 2}\n\n\npage_content='EEP3010: Communication Systems Lab  \\n \\nFunctional block diagram of DCL -QAM kit  \\n \\nEXPT  A – TRIBIT DAT A CODING TECHNIQUE OF NRZ -L DAT A FORMAT  \\n1. Refer to the block diagram and carry out the following connections and \\nswitch settings.  \\n2. Connect power supply in proper polarity to the kit DCL -QAM and switch it \\non. \\n3. Select Data pattern of simulated data using switch SW1, SW2, SW3.  \\n4. Connect SERIAL DATA generated to DATA IN of TRIBIT ENCODER.  \\n5. Observe the tribit clock generated at EVEN CLK, ODD CLK and C CLK on \\nthe oscilloscope with respect to DATA_CLOCK.  \\n6. Observe the coded signal EVEN, ODD and C data on the oscilloscope with \\nrespect to SERIAL DATA.  \\n \\n' metadata={'source': '/kaggle/input/infodata/Soham_Deshmukh_B21EE067_Exp_5.pdf', 'page': 3}\n\n\npage_content='EEP3010: Communication Systems Lab  \\n \\nEXPT  B – DIFFERENTIAL ENCODING OF DAT A  \\n1. Refer to the block diagram and carry out the following connections and \\nswitch settings.  \\n2. Connect power supply in proper polarity to the kit DCL -QAM and switch it \\non. \\n3. Select Data pattern of simulated data using switch SW1, SW2, SW3.  \\n4. Connect SERIAL DATA generated to DATA IN of TRIBIT ENCODER.  \\n5. Connect generated EVEN and ODD data to DATA IN1and DATA IN2 of \\nDIFFERENTIAL ENCODERS respectively.  \\n6. Observe the input data and the differentially encoded EVEN & ODD data at \\nOUT1 and OUT2 of DIFFERENTIAL ENCODERS respectively.  \\n \\n \\n \\n \\nEXPT  C – OBSERVATION OF THE CONSTELLATION DIAGRAM FOR QAM \\nAND STUDY  OF BANDWIDTH EFFICIENCY OF QAM  \\n1. Study of constellation diagram  -  \\na. Refer to the block diagram (Fig 3.1) and carry out the following \\nconnections and  switch settings.  \\nb. Connect power supply in proper polarity to the kit DCL -QAM and \\nswitch it on.  \\nc. Select Data pattern of simulated data using switch SW1, SW2, SW3.  \\nd. Connect SERIAL DATA generated to DATA IN of the TRIBIT \\nENCODER.  \\ne. Now connect X & Y port of CONSTELLATION BLOCK to X -channel \\nand Y-channel of CRO respectively  \\nf. Observe the waveforms.  \\n \\n' metadata={'source': '/kaggle/input/infodata/Soham_Deshmukh_B21EE067_Exp_5.pdf', 'page': 4}\n\n\n","output_type":"stream"}]},{"cell_type":"code","source":"text_splitter = RecursiveCharacterTextSplitter(\n                chunk_size = 1500,\n                chunk_overlap = 150)\n\nsplits = text_splitter.split_documents(docs)","metadata":{"execution":{"iopub.status.busy":"2024-06-08T09:24:51.620973Z","iopub.execute_input":"2024-06-08T09:24:51.621443Z","iopub.status.idle":"2024-06-08T09:24:51.631295Z","shell.execute_reply.started":"2024-06-08T09:24:51.621401Z","shell.execute_reply":"2024-06-08T09:24:51.630182Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"from langchain.embeddings.openai import OpenAIEmbeddings\n\nembedding = OpenAIEmbeddings()\n\ns1 = \"i like dogs\"\ns2 = \"i like canines\"\ns3 = \"the weather is ugly outside\"\n\ne1 = embedding.embed_query(s1)\ne2 = embedding.embed_query(s2)\ne3 = embedding.embed_query(s3)\n\n'''To find similarity'''\nsimilarity = np.dot(e1, e2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"''' VECTOR STORE '''\n\nfrom langchain.vectorstores import Chroma\n\npersist_dir = '/kaggle/working/chroma/'","metadata":{"execution":{"iopub.status.busy":"2024-06-08T09:24:51.841508Z","iopub.execute_input":"2024-06-08T09:24:51.841882Z","iopub.status.idle":"2024-06-08T09:24:51.859179Z","shell.execute_reply.started":"2024-06-08T09:24:51.841849Z","shell.execute_reply":"2024-06-08T09:24:51.858085Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"!rm -rf ./kaggle/working/chroma","metadata":{"execution":{"iopub.status.busy":"2024-06-08T09:24:51.860556Z","iopub.execute_input":"2024-06-08T09:24:51.860880Z","iopub.status.idle":"2024-06-08T09:24:52.881576Z","shell.execute_reply.started":"2024-06-08T09:24:51.860851Z","shell.execute_reply":"2024-06-08T09:24:52.880030Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"vectordb = Chroma.from_documents(\n            documents = splits,\n            embedding = embedding,\n            persist_directory = persist_dir\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"question = 'Ask some question realted to the text.'\ndocs = vectordb.similarity_search(question, k=3)    # k= means return 3 documents as output\n\nprint(docs[0].page_content)\n\nvector_db.persist()            # makes db fixed for further use -- no loss","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# RETRIEVAL","metadata":{}},{"cell_type":"code","source":"''' Important for query time - retrieve most relevant split \n\n1) Maximum Marginal Relevance (MMR)\n    - not select the most similar responses \n    - select more important info not relevant\n    - create more diverse responses\n    - 'fetch_k' parameter - k responses\n    \n2) LLM Aided Retrieval\n    - Query is more than just the Question\n    - LLM is used to convert question to query\n    - Query contains - Filter (metadata) + Search Term\n    \n3) Compression\n    - Shrink the responses to only the relevant info\n    - Use Compression LLM to get most relevant parts\n    \n'''","metadata":{"execution":{"iopub.status.busy":"2024-06-08T09:24:53.247454Z","iopub.status.idle":"2024-06-08T09:24:53.247886Z","shell.execute_reply.started":"2024-06-08T09:24:53.247694Z","shell.execute_reply":"2024-06-08T09:24:53.247712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"texts = [\n    \"\"\"The death cap has a large and imposing epigeous (aboveground) fruiting body (basidiocarp), usually with a pileus (cap) from 5 to 15 centimetres (2 to 5+7⁄8 inches) across, initially rounded and hemispherical, but flattening with age. The color of the cap can be pale-green, yellowish-green, olive-green, bronze, or (in one form) white; it is often paler toward the margins, which can have darker streaks; it is also often paler after rain. The cap surface is sticky when wet and easily peeled—a troublesome feature, as that is allegedly a feature of edible fungi.\"\"\", \n    \"\"\"The remains of the partial veil are seen as a skirtlike, floppy annulus usually about 1 to 1.5 cm (3⁄8 to 5⁄8 in) below the cap. The crowded white lamellae (gills) are free. The stipe is white with a scattering of grayish-olive scales and is 8 to 15 cm (3+1⁄8 to 5+7⁄8 in) long and 1 to 2 cm (3⁄8 to 3⁄4 in) thick, with a swollen, ragged, sac-like white volva (base). As the volva, which may be hidden by leaf litter, is a distinctive and diagnostic feature, it is important to remove some debris to check for it. Spores: 7-12 x 6-9 μm. Smooth, ellipsoid, amyloid.\"\"\",\n    \"\"\"The smell has been described as initially faint and honey-sweet, but strengthening over time to become overpowering, sickly-sweet and objectionable.[32] Young specimens first emerge from the ground resembling a white egg covered by a universal veil, which then breaks, leaving the volva as a remnant. The spore print is white, a common feature of Amanita. The transparent spores are globular to egg-shaped, measure 8–10 μm (0.3–0.4 mil) long, and stain blue with iodine. The gills, in contrast, stain pallid lilac or pink with concentrated sulfuric acid.\"\"\"\n]\n\nsmalldb = Chroma.from_texts(texts, embedding = embedding)\n\nquestion = \"Tell me abput all-white mushrooms with large fruiting bodies\"\nsmalldb.similarity_search(question, k=2)\n\n''' MMR '''\nsmall_db.max_marginal_relevance_search(question, k=2, fetch_k=3)","metadata":{"execution":{"iopub.status.busy":"2024-06-08T09:24:53.249669Z","iopub.status.idle":"2024-06-08T09:24:53.250143Z","shell.execute_reply.started":"2024-06-08T09:24:53.249921Z","shell.execute_reply":"2024-06-08T09:24:53.249939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"''' GETTING RELEVANT DOCUMENTS FIRST '''\n\ndocs = vectordb.similarity_search(question, k=3, filter={\"source\":\"docs/...\"})\n\nfor d in docs:\n    print(d.metadata)","metadata":{"execution":{"iopub.status.busy":"2024-06-08T09:24:53.251348Z","iopub.status.idle":"2024-06-08T09:24:53.251755Z","shell.execute_reply.started":"2024-06-08T09:24:53.251579Z","shell.execute_reply":"2024-06-08T09:24:53.251596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from langchain.llms import OpenAI\nfrom langchain.retrievers.self_query.base import SelfQueryRetriever\nfrom langchain.chains.query_constructor.base import AttributeInfo\n\nmetadata_field_info = [\n    AtrributeInfo{\n        name=\"source\",\n        description=\"The lecture the chunk is from, should be from one of 'docs/...'\",\n        type=\"string\",\n    },\n    AttributeInfo{\n        name=\"page\",\n        description=\"The oage from the lecture\",\n        type=\"string\"\n    }\n]\n\ndocument_content_description = \"Notes\"\nllm = OpenAI(temperature=0)\n\nretriever = SelfQueryRetriever.from_llm(\n    llm,\n    vectordb,\n    document_content_description,\n    metadata_field_info,\n    verbose=True\n)\n\nquestion = \"What do they say about regression in the third lecture?\"\n\ndocs = retriever.get_relevant_documents(question)\n\nfor doc in docs:\n    print(doc.metadata)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-08T09:24:53.253768Z","iopub.status.idle":"2024-06-08T09:24:53.254192Z","shell.execute_reply.started":"2024-06-08T09:24:53.254003Z","shell.execute_reply":"2024-06-08T09:24:53.254020Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"''' CONTEXTUAL CHAIN RETRIEVAL '''\n\nfrom langchain.retrievers import ContextualCompressionRetriever\nfrom langchain.retrievers.document_compressors import LLMChainExtractor\n\nllm = OpenAI(temperature = 0)\ncompressor = LLMChainExtractor.from_llm(llm)\n\ncompression_retriever = ContextualCompressionRetriever(\n    base_compressor = compressor,\n    base_retirever = vectordb.as_retriever(search_type='mmr')          # MMR - to remove repeating info\n)\n\nquestion = \"what did they say about matlab ?\"\ncompressed_docs = compression_retriever.get_relevant_documents(question)\n\n# print(compressed_docs)","metadata":{"execution":{"iopub.status.busy":"2024-06-08T09:24:53.256393Z","iopub.status.idle":"2024-06-08T09:24:53.256800Z","shell.execute_reply.started":"2024-06-08T09:24:53.256618Z","shell.execute_reply":"2024-06-08T09:24:53.256634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from langchain.retrievers import SVMRetriever, TFIDFRetriever\nfrom langchain.langchain.document_loaders import PyPDFLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\nloader = PyPDFLoader(\"../address/..\")\npages = loader.load()\n\nall_pages_text = [p.page_content for p in pages]\njoined_page_text = \" \".join(all_page_text)\n\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size = 1500, chunk_overlap=50)\nsplits = text_splitter.split_text(joined_page_text)\n\nsvm_retriever = SVMRetriever.from_texts(splits, embeddings)\ntfidf_retriever = TFIDFRetriever.from_texts(splits)\n\nquestion = \"what do they say about matlab ?\"\n\ndocs_svm = svm_retriever.get_relevant_documents(question)\ndocs_tfidf = tfidf_retriever.get_relevant_documents(question)","metadata":{"execution":{"iopub.status.busy":"2024-06-08T09:24:53.258205Z","iopub.status.idle":"2024-06-08T09:24:53.258613Z","shell.execute_reply.started":"2024-06-08T09:24:53.258428Z","shell.execute_reply":"2024-06-08T09:24:53.258445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# QUESTION ANSWERING","metadata":{}},{"cell_type":"code","source":"import os\nimport openai\nimport sys\n\nsys.path.append('../..')\n\nfrom dotenv import load_dotenv, find_dotenv\n_ = load_dotenv(find_dotenv())\n\nopenai.api_key = os.environ['OPENAI_API_KEY']\n\nfrom langchain.vectorstores import Chroma\nfrom langchain.embeddings.openai import OpenAIEmbeddings\n\npersist_directory = 'docs/chroma/'\nembedding  = OpenAIEmbeddings()\n\nvectordb = Chroma(persist_directory = persist_directory , embedding_function = embedding)\nprint(vectordb._collection.count())","metadata":{"execution":{"iopub.status.busy":"2024-06-08T09:24:53.259758Z","iopub.status.idle":"2024-06-08T09:24:53.260113Z","shell.execute_reply.started":"2024-06-08T09:24:53.259938Z","shell.execute_reply":"2024-06-08T09:24:53.259953Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from langchain.chat_models import ChatOpenAI\nfrom langchain.chains import RetrievalQAqa\n\nllm = ChatOpenAI(model_name = 'gpt-3.5-turbo', temperature=0)\n\nqa_chain = RetrievalQA.from_chain_type(llm, retriever=vectordb.as_retriever())\n\nresult = qa_chain({\"query\":question})\nresult[\"result\"]","metadata":{"execution":{"iopub.status.busy":"2024-06-08T09:24:53.261468Z","iopub.status.idle":"2024-06-08T09:24:53.261825Z","shell.execute_reply.started":"2024-06-08T09:24:53.261652Z","shell.execute_reply":"2024-06-08T09:24:53.261667Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from langchain.prompts import PromptTemplate\n\ntemplate = \"\"\"Use the following pieces of context to answer the question\n{context}\nQuestion: {question}\nHelpful answer:\n\"\"\"\n\nQA_chain_prompt = PromptTemplate.from_template(template)\n\nqa_chain = RetrievalQA.from_chain_type(\n    llm,\n    retriever= vectordb.as_retriever(),\n    return_source_documents = True,\n    chain_type_kwargs = {\"prompt\":QA_chain_prompt}\n)\n\nquestion = \"Is probability a class topic?\"\nresult = qa_chain({\"prompt\":question})\n\nprint(result[\"Result\"])\nprint(result[\"source_documents\"])","metadata":{"execution":{"iopub.status.busy":"2024-06-08T09:24:53.263073Z","iopub.status.idle":"2024-06-08T09:24:53.263452Z","shell.execute_reply.started":"2024-06-08T09:24:53.263250Z","shell.execute_reply":"2024-06-08T09:24:53.263265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"''' What happens inside a Lang QA chain ??'''\n# Document chain - multiple calls to Lang model\n# Input output for each doc\n# All responses stuffed together\n# Combination of all responses -- given as output","metadata":{"execution":{"iopub.status.busy":"2024-06-08T09:24:53.265242Z","iopub.status.idle":"2024-06-08T09:24:53.265641Z","shell.execute_reply.started":"2024-06-08T09:24:53.265458Z","shell.execute_reply":"2024-06-08T09:24:53.265475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Memory Element in Retrieval Chains\n### How can the chain remember data from previous responses ? --> some storage.","metadata":{}},{"cell_type":"code","source":"from langchain.memory import ConversationBufferMemory\n\nmemory = ConversationBufferMemory(\n    memory_key = \"chat_history\",\n    return_messages = True\n)\n# returns a list of chats","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from langchain.chains import ConversationalRetrievalChain\n\nretriever = vectordb.as_retriever()\nqa = ConversationRetrievalChain.from_llm(\n    llm,\n    retriever = retriever,\n    memory = memory\n)\n\nquestion = \"Is probability a class topic?\"\nresult =  qa({\"question\":question})\nprint(result[\"Answer\"])\n\nquestion = \"Why are those prerequisites necessary?\"\nresult =  qa({\"question\":question})\nprint(result[\"Answer\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}